{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015d0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# Imports for server connection\n",
    "import socket\n",
    "from send_receive import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec9d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "      self.lin1 = nn.Linear(784, 256)\n",
    "      self.lin2 = nn.Linear(256, 64)\n",
    "      self.lin3 = nn.Linear(64, 10)\n",
    "\n",
    "  def forward(self, X):\n",
    "      x1 = F.relu(self.lin1(X))\n",
    "      x2 = F.relu(self.lin2(x1))\n",
    "      x3 = self.lin3(x2)\n",
    "      return x3\n",
    "\n",
    "  # Fit function\n",
    "  def fit(self, X, y, optimizer, loss_fn, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "      ypred = self.forward(X)\n",
    "      loss = loss_fn(ypred, y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70ed0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 784])\n"
     ]
    }
   ],
   "source": [
    "# Data fetching\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "# Use first 50,000 entries from mnist training set, rest are for server\n",
    "X_train = mnist_trainset.data[:50000,:]\n",
    "X_train = X_train.float().flatten(start_dim=1, end_dim=2) # Flatten training images\n",
    "Y_train = mnist_trainset.targets[:50000]\n",
    "\n",
    "# Load testsset\n",
    "X_test = mnist_testset.data\n",
    "X_test = X_test.float().flatten(start_dim=1, end_dim=2) # Flatten test images\n",
    "Y_test = mnist_testset.targets\n",
    "\n",
    "#X_train.shape\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59854674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CronusKLLoss(nn.Module):\n",
    "    def __init__(self, T=3.0):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits):\n",
    "        student_log_probs = F.log_softmax(student_logits / self.T, dim=1)\n",
    "        teacher_probs     = F.softmax(teacher_logits / self.T, dim=1)\n",
    "        return self.kl(student_log_probs, teacher_probs) * (self.T ** 2)\n",
    "\n",
    "class SoftKLLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, student_logits, teacher_probs):\n",
    "        student_log_probs = F.log_softmax(student_logits, dim=1)\n",
    "        return self.kl(student_log_probs, teacher_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a96937fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import socket\n",
    "\n",
    "\n",
    "def start_clients(HOST, PORT, num_models, initialization_epochs, collab_epochs):\n",
    "\n",
    "    sampleSize = len(X_train) // num_models\n",
    "    imgSize = len(X_train[0])\n",
    "\n",
    "    X_trains = torch.zeros((sampleSize, imgSize, num_models))\n",
    "    Y_trains = torch.zeros((sampleSize, num_models))\n",
    "\n",
    "    # Fill data for each model\n",
    "    for m in range(num_models):\n",
    "        idx = torch.randperm(len(X_train))[:sampleSize]\n",
    "        X_trains[:, :, m] = X_train[idx]\n",
    "        Y_trains[:, m] = Y_train[idx]\n",
    "\n",
    "    # Socket setup\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.connect((HOST, PORT))\n",
    "    print(f\"Connected to {HOST} on port {PORT}\")\n",
    "\n",
    "    try:\n",
    "        # Receive public features\n",
    "        features = recv_tensor(s)\n",
    "        print(features.shape)\n",
    "    \n",
    "        # Create models\n",
    "        models = [MnistModel() for _ in range(num_models)]\n",
    "    \n",
    "        # INITIALIZATION\n",
    "        optimAdams = [Adam(models[i].parameters(), lr=0.0005) for i in range(num_models)]\n",
    "        for i in range(num_models):\n",
    "            models[i].fit(\n",
    "                X_trains[:, :, i],\n",
    "                Y_trains[:, i].long(),\n",
    "                optimAdams[i],\n",
    "                nn.CrossEntropyLoss(),\n",
    "                initialization_epochs\n",
    "            )\n",
    "    \n",
    "        # Logging\n",
    "        for j in range(num_models):\n",
    "            preds = models[j].forward(X_test).argmax(dim=1)\n",
    "            err = (preds != Y_test).float().mean()\n",
    "            print(f\"Initialization, model {j}, error {err}\")\n",
    "    \n",
    "        # Initial predictions\n",
    "        predictions = torch.stack(\n",
    "            [models[i].forward(features).detach() for i in range(num_models)],\n",
    "            dim=0\n",
    "        )\n",
    "        send_tensor(s, predictions)\n",
    "    \n",
    "        aggregationLabels = recv_tensor(s)\n",
    "        aggregationLabels = F.softmax(aggregationLabels, dim=1)\n",
    "    \n",
    "        # COLLABORATION\n",
    "\n",
    "        optimSGDs = [SGD(models[j].parameters(), lr=0.001) for j in range(num_models)]\n",
    "\n",
    "        optim_hard = [\n",
    "        torch.optim.SGD(models[j].parameters(), lr=0.01)\n",
    "        for j in range(num_models)\n",
    "        ]\n",
    "    \n",
    "        optim_soft = [\n",
    "            torch.optim.SGD(models[j].parameters(), lr=0.001)\n",
    "            for j in range(num_models)\n",
    "        ]\n",
    "        \n",
    "        for t in range(collab_epochs):\n",
    "    \n",
    "            predictions = []\n",
    "    \n",
    "            for j in range(num_models):\n",
    "\n",
    "                # -------- 1. HARD update (anchor) --------\n",
    "                models[j].fit(\n",
    "                    X_trains[:, :, j],\n",
    "                    Y_trains[:, j].long(),\n",
    "                    optim_hard[j],\n",
    "                    nn.CrossEntropyLoss(),\n",
    "                    epochs=1\n",
    "                )\n",
    "            \n",
    "                # -------- 2. SOFT update (regularizer) --------\n",
    "                models[j].fit(\n",
    "                    features,\n",
    "                    aggregationLabels.detach(),\n",
    "                    optim_soft[j],\n",
    "                    SoftKLLoss(),\n",
    "                    epochs=1\n",
    "                )\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                Y_local = F.one_hot(\n",
    "                    Y_trains[..., j].long(),\n",
    "                    num_classes=10\n",
    "                ).float()\n",
    "\n",
    "                X_mix = torch.cat([X_trains[..., j], features])\n",
    "                Y_mix = torch.cat([Y_local, aggregationLabels.detach()])\n",
    "                \n",
    "                models[j].fit(X_mix, Y_mix, optimSGDs[j], SoftKLLoss(), epochs=5)\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                \"\"\"\n",
    "                models[j].fit(X_trains[:, :, j], Y_trains[:, j].long(),\n",
    "                          optimSGD, nn.CrossEntropyLoss(), epochs=5)\n",
    "    \n",
    "                # One-step distillation update\n",
    "                models[j].fit(\n",
    "                    features,\n",
    "                    aggregationLabels.detach(),\n",
    "                    optimSGD,\n",
    "                    CronusKLLoss(T=3.0),\n",
    "                    epochs=1\n",
    "                )\n",
    "                \"\"\"\n",
    "    \n",
    "                predictions.append(models[j].forward(features).detach())\n",
    "    \n",
    "            predictions = torch.stack(predictions, dim=0)\n",
    "            send_tensor(s, predictions)\n",
    "    \n",
    "            aggregationLabels = recv_tensor(s)\n",
    "            aggregationLabels = F.softmax(aggregationLabels, dim=1)\n",
    "    \n",
    "            # Logging\n",
    "            for j in range(num_models):\n",
    "                preds = models[j].forward(X_test).argmax(dim=1)\n",
    "                err = (preds != Y_test).float().mean()\n",
    "                print(f\"Collab step {t}, model {j}, error {err}\")\n",
    "    \n",
    "        print(\"Finished\")\n",
    "    \n",
    "        s.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        s.close()\n",
    "        print(\"Keyboard Interrupt, Thread Closed\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fd157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to localhost on port 65435\n",
      "torch.Size([10000, 784])\n",
      "Initialization, model 0, error 0.09080000221729279\n",
      "Initialization, model 1, error 0.09390000253915787\n",
      "Initialization, model 2, error 0.10320000350475311\n",
      "Initialization, model 3, error 0.1023000031709671\n",
      "Initialization, model 4, error 0.10289999842643738\n",
      "Initialization, model 5, error 0.09749999642372131\n",
      "Initialization, model 6, error 0.0917000025510788\n",
      "Initialization, model 7, error 0.09870000183582306\n",
      "Initialization, model 8, error 0.09189999848604202\n",
      "Initialization, model 9, error 0.09200000017881393\n",
      "Collab step 0, model 0, error 0.08919999748468399\n",
      "Collab step 0, model 1, error 0.09359999746084213\n",
      "Collab step 0, model 2, error 0.10360000282526016\n",
      "Collab step 0, model 3, error 0.10209999978542328\n",
      "Collab step 0, model 4, error 0.10180000215768814\n",
      "Collab step 0, model 5, error 0.09740000218153\n",
      "Collab step 0, model 6, error 0.09160000085830688\n",
      "Collab step 0, model 7, error 0.09669999778270721\n",
      "Collab step 0, model 8, error 0.09099999815225601\n",
      "Collab step 0, model 9, error 0.0917000025510788\n",
      "Collab step 1, model 0, error 0.08980000019073486\n",
      "Collab step 1, model 1, error 0.09510000050067902\n",
      "Collab step 1, model 2, error 0.10119999945163727\n",
      "Collab step 1, model 3, error 0.10130000114440918\n",
      "Collab step 1, model 4, error 0.10040000081062317\n",
      "Collab step 1, model 5, error 0.09679999947547913\n",
      "Collab step 1, model 6, error 0.09160000085830688\n",
      "Collab step 1, model 7, error 0.09719999879598618\n",
      "Collab step 1, model 8, error 0.09130000323057175\n",
      "Collab step 1, model 9, error 0.09130000323057175\n",
      "Collab step 2, model 0, error 0.08810000121593475\n",
      "Collab step 2, model 1, error 0.09359999746084213\n",
      "Collab step 2, model 2, error 0.10270000249147415\n",
      "Collab step 2, model 3, error 0.10140000283718109\n",
      "Collab step 2, model 4, error 0.09989999979734421\n",
      "Collab step 2, model 5, error 0.09700000286102295\n",
      "Collab step 2, model 6, error 0.09019999951124191\n",
      "Collab step 2, model 7, error 0.09589999914169312\n",
      "Collab step 2, model 8, error 0.08990000188350677\n",
      "Collab step 2, model 9, error 0.09139999747276306\n",
      "Collab step 3, model 0, error 0.08919999748468399\n",
      "Collab step 3, model 1, error 0.09560000151395798\n",
      "Collab step 3, model 2, error 0.10249999910593033\n",
      "Collab step 3, model 3, error 0.10490000247955322\n",
      "Collab step 3, model 4, error 0.09920000284910202\n",
      "Collab step 3, model 5, error 0.09700000286102295\n",
      "Collab step 3, model 6, error 0.09080000221729279\n",
      "Collab step 3, model 7, error 0.09780000150203705\n",
      "Collab step 3, model 8, error 0.09040000289678574\n",
      "Collab step 3, model 9, error 0.09059999883174896\n",
      "Collab step 4, model 0, error 0.08879999816417694\n",
      "Collab step 4, model 1, error 0.09610000252723694\n",
      "Collab step 4, model 2, error 0.10480000078678131\n",
      "Collab step 4, model 3, error 0.10350000113248825\n",
      "Collab step 4, model 4, error 0.09860000014305115\n",
      "Collab step 4, model 5, error 0.0989999994635582\n",
      "Collab step 4, model 6, error 0.08889999985694885\n",
      "Collab step 4, model 7, error 0.09950000047683716\n",
      "Collab step 4, model 8, error 0.08900000154972076\n",
      "Collab step 4, model 9, error 0.09109999984502792\n",
      "Collab step 5, model 0, error 0.08950000256299973\n",
      "Collab step 5, model 1, error 0.10080000013113022\n",
      "Collab step 5, model 2, error 0.10939999669790268\n",
      "Collab step 5, model 3, error 0.11219999939203262\n",
      "Collab step 5, model 4, error 0.1006999984383583\n",
      "Collab step 5, model 5, error 0.10480000078678131\n",
      "Collab step 5, model 6, error 0.08959999680519104\n",
      "Collab step 5, model 7, error 0.10849999636411667\n",
      "Collab step 5, model 8, error 0.08910000324249268\n",
      "Collab step 5, model 9, error 0.09070000052452087\n",
      "Collab step 6, model 0, error 0.09210000187158585\n",
      "Collab step 6, model 1, error 0.1062999963760376\n",
      "Collab step 6, model 2, error 0.11710000038146973\n",
      "Collab step 6, model 3, error 0.11079999804496765\n",
      "Collab step 6, model 4, error 0.09910000115633011\n",
      "Collab step 6, model 5, error 0.10790000110864639\n",
      "Collab step 6, model 6, error 0.08959999680519104\n",
      "Collab step 6, model 7, error 0.11569999903440475\n",
      "Collab step 6, model 8, error 0.0892999991774559\n",
      "Collab step 6, model 9, error 0.09070000052452087\n",
      "Collab step 7, model 0, error 0.09260000288486481\n",
      "Collab step 7, model 1, error 0.11680000275373459\n",
      "Collab step 7, model 2, error 0.1151999980211258\n",
      "Collab step 7, model 3, error 0.1324000060558319\n",
      "Collab step 7, model 4, error 0.10530000180006027\n",
      "Collab step 7, model 5, error 0.12189999967813492\n",
      "Collab step 7, model 6, error 0.09009999781847\n",
      "Collab step 7, model 7, error 0.15279999375343323\n",
      "Collab step 7, model 8, error 0.08869999647140503\n",
      "Collab step 7, model 9, error 0.09260000288486481\n",
      "Collab step 8, model 0, error 0.09529999643564224\n",
      "Collab step 8, model 1, error 0.13040000200271606\n",
      "Collab step 8, model 2, error 0.11949999630451202\n",
      "Collab step 8, model 3, error 0.13220000267028809\n",
      "Collab step 8, model 4, error 0.10119999945163727\n",
      "Collab step 8, model 5, error 0.11999999731779099\n",
      "Collab step 8, model 6, error 0.09220000356435776\n",
      "Collab step 8, model 7, error 0.20579999685287476\n",
      "Collab step 8, model 8, error 0.08990000188350677\n",
      "Collab step 8, model 9, error 0.09359999746084213\n",
      "Collab step 9, model 0, error 0.09730000048875809\n",
      "Collab step 9, model 1, error 0.15780000388622284\n",
      "Collab step 9, model 2, error 0.11860000342130661\n",
      "Collab step 9, model 3, error 0.17000000178813934\n",
      "Collab step 9, model 4, error 0.11590000241994858\n",
      "Collab step 9, model 5, error 0.12319999933242798\n",
      "Collab step 9, model 6, error 0.09520000219345093\n",
      "Collab step 9, model 7, error 0.22040000557899475\n",
      "Collab step 9, model 8, error 0.08980000019073486\n",
      "Collab step 9, model 9, error 0.10159999877214432\n",
      "Collab step 10, model 0, error 0.09740000218153\n",
      "Collab step 10, model 1, error 0.18610000610351562\n",
      "Collab step 10, model 2, error 0.13050000369548798\n",
      "Collab step 10, model 3, error 0.16349999606609344\n",
      "Collab step 10, model 4, error 0.11110000312328339\n",
      "Collab step 10, model 5, error 0.12809999287128448\n",
      "Collab step 10, model 6, error 0.10329999774694443\n",
      "Collab step 10, model 7, error 0.18140000104904175\n",
      "Collab step 10, model 8, error 0.09009999781847\n",
      "Collab step 10, model 9, error 0.10930000245571136\n",
      "Collab step 11, model 0, error 0.10589999705553055\n",
      "Collab step 11, model 1, error 0.210099995136261\n",
      "Collab step 11, model 2, error 0.125900000333786\n",
      "Collab step 11, model 3, error 0.18279999494552612\n",
      "Collab step 11, model 4, error 0.13330000638961792\n",
      "Collab step 11, model 5, error 0.13420000672340393\n",
      "Collab step 11, model 6, error 0.11079999804496765\n",
      "Collab step 11, model 7, error 0.1420000046491623\n",
      "Collab step 11, model 8, error 0.08900000154972076\n",
      "Collab step 11, model 9, error 0.12720000743865967\n",
      "Collab step 12, model 0, error 0.10580000281333923\n",
      "Collab step 12, model 1, error 0.1956000030040741\n",
      "Collab step 12, model 2, error 0.13269999623298645\n",
      "Collab step 12, model 3, error 0.13510000705718994\n",
      "Collab step 12, model 4, error 0.16060000658035278\n",
      "Collab step 12, model 5, error 0.13899999856948853\n",
      "Collab step 12, model 6, error 0.13840000331401825\n",
      "Collab step 12, model 7, error 0.11429999768733978\n",
      "Collab step 12, model 8, error 0.0892999991774559\n",
      "Collab step 12, model 9, error 0.15440000593662262\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "HOST = \"localhost\"\n",
    "PORT = 65435\n",
    "\n",
    "num_models = 10\n",
    "initialization_epochs = 50\n",
    "collab_epochs = 50\n",
    "\n",
    "start_clients(\n",
    "    HOST,\n",
    "    PORT,\n",
    "    num_models,\n",
    "    initialization_epochs,\n",
    "    collab_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e234e1c6-e5c9-48d9-a6dd-b2f4db73c83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
