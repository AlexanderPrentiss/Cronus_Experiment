{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6460dea",
   "metadata": {},
   "source": [
    "### Limit Scope to Only AGR Testing, No Sockets, No Implementation, Just Logits Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16792262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.datasets as datasets\n",
    "from scipy.stats import norm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7858be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "class RunningStats:\n",
    "    \"\"\"\n",
    "    Tracks running mean and covariance of vectors in R^C\n",
    "    using online (streaming) updates.\n",
    "\n",
    "    This is purely observational — no modification of inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, device=\"cpu\", eps=1e-6):\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "        self.eps = eps\n",
    "\n",
    "        self.n = 0\n",
    "        self.mean = torch.zeros(dim, device=device)\n",
    "        self.M2 = torch.zeros(dim, dim, device=device)  # sum of outer products\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (C,)\n",
    "        \"\"\"\n",
    "        self.n += 1\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = x - self.mean\n",
    "        self.M2 += torch.outer(delta, delta2)\n",
    "\n",
    "    def covariance(self):\n",
    "        if self.n < 2:\n",
    "            return torch.eye(self.dim, device=self.device) * self.eps\n",
    "        return self.M2 / (self.n - 1) + self.eps * torch.eye(self.dim, device=self.device)\n",
    "\n",
    "    def principal_eigensystem(self):\n",
    "        Sigma = self.covariance()\n",
    "        eigvals, eigvecs = torch.linalg.eigh(Sigma)\n",
    "        return eigvals[-1], eigvecs[:, -1]\n",
    "\n",
    "    def std(self):\n",
    "        \"\"\"Per-class std (shape: C)\"\"\"\n",
    "        return torch.sqrt(torch.diag(self.covariance()))\n",
    "\n",
    "\n",
    "class PublicPredictionObserver:\n",
    "    \"\"\"\n",
    "    Observes public predictions over rounds and maintains\n",
    "    per-sample statistics if desired.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, device=\"cpu\"):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.stats = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.stats = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def observe(self, predictions):\n",
    "        \"\"\"\n",
    "        predictions: Tensor of shape (N, C)\n",
    "        \"\"\"\n",
    "        N, C = predictions.shape\n",
    "        assert C == self.num_classes\n",
    "\n",
    "        if self.stats is None:\n",
    "            self.stats = [\n",
    "                RunningStats(C, device=self.device)\n",
    "                for _ in range(N)\n",
    "            ]\n",
    "\n",
    "        for i in range(N):\n",
    "            self.stats[i].update(predictions[i])\n",
    "\n",
    "    def mean(self):\n",
    "        return torch.stack([s.mean for s in self.stats])\n",
    "    \n",
    "    def std(self):\n",
    "        return torch.stack([s.std() for s in self.stats])\n",
    "    \n",
    "    def covariance(self, idx):\n",
    "        \"\"\"\n",
    "        Covariance for a specific public sample\n",
    "        \"\"\"\n",
    "        return self.stats[idx].covariance()\n",
    "\n",
    "    def dominant_direction(self, idx):\n",
    "        return self.stats[idx].principal_eigensystem()\n",
    "\n",
    "    \n",
    "def inv_phi(n):\n",
    "    if not (0.0 < n < 1.0):\n",
    "        raise ValueError(\"n must be in (0, 1)\")\n",
    "    return norm.ppf(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959ca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BENIGN CLIENT\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(784, 256)\n",
    "        self.lin2 = nn.Linear(256, 64)\n",
    "        self.lin3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        return self.lin3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6a35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALICIOUS CLIENT\n",
    "class LIE_Model(nn.Module):\n",
    "    def __init__(self, active_round, num_malicious, num_models, observer):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(784, 256)\n",
    "        self.lin2 = nn.Linear(256, 64)\n",
    "        self.lin3 = nn.Linear(64, 10)\n",
    "        self.active_round = active_round\n",
    "        self.num_rounds = 0\n",
    "        self.observer = observer\n",
    "        self.num_malicious = num_malicious\n",
    "        self.num_models= num_models\n",
    "\n",
    "    def calculate_v(self):\n",
    "        mu = self.observer.mean()      \n",
    "        sigma = self.observer.std()     \n",
    "        z = inv_phi(1.0 - self.num_malicious / self.num_models)\n",
    "        v = mu - z * sigma\n",
    "\n",
    "        return v\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training and self.num_rounds > self.active_round:\n",
    "            return self.calculate_v()\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        return self.lin3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b8c428-902d-4f4a-88ea-45742cc95cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ce_fullbatch(model, X, y, optimizer, epochs):\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def train_distill_fullbatch(model, X_pub, teacher_probs, optimizer, epochs, lam = 1, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Distill using teacher probability vectors (Cronus shares prediction vectors).\n",
    "    Minimize KL(teacher || student) equivalent to cross-entropy with soft targets.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        student_logits = model(X_pub)\n",
    "        student_log_probs = F.log_softmax(student_logits, dim=1)\n",
    "        # KLDivLoss expects log-probs input and probs target\n",
    "        loss = lam * F.kl_div(student_log_probs, teacher_probs.clamp_min(eps), reduction=\"batchmean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs(model, X):\n",
    "    model.eval()\n",
    "    probs = F.softmax(model(X), dim=1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7428992f-6d68-40b5-95aa-31f93e83ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Median Aggregation\n",
    "# - Simple median of logit predictions\n",
    "# - Used as a baseline\n",
    "# -----------------------------\n",
    "def f_median(logits):\n",
    "\n",
    "    mu = logits.median(dim=0).values\n",
    "\n",
    "    return mu\n",
    "\n",
    "# -----------------------------\n",
    "# Mean Aggregation\n",
    "# - Simple mean of logit predictions\n",
    "# - Used as a baseline\n",
    "# -----------------------------\n",
    "def f_mean(logits):\n",
    "\n",
    "    mu = logits.mean(dim=0)\n",
    "\n",
    "    return mu\n",
    "\n",
    "# -----------------------------\n",
    "# RobustFilter Aggregation\n",
    "# - Trims logits\n",
    "# -----------------------------\n",
    "def f_cronus(\n",
    "    logits,\n",
    "    eps=1e-3,\n",
    "    lambda_thresh=9.0,\n",
    "    max_iters=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust Cronus aggregation.\n",
    "\n",
    "    logits: Tensor [K, N, C]  (models × samples × classes)\n",
    "    returns: Tensor [N, C]\n",
    "    \"\"\"\n",
    "\n",
    "    K, N, C = logits.shape\n",
    "    device = logits.device\n",
    "\n",
    "    agg = torch.zeros(N, C, device=device)\n",
    "\n",
    "    for n in range(N):\n",
    "\n",
    "        # Y: [K, C] logits for sample n\n",
    "        Y = logits[:, n, :]\n",
    "\n",
    "        # Initial mean\n",
    "        mu = Y.mean(dim=0)\n",
    "\n",
    "        for _ in range(max_iters):\n",
    "\n",
    "            # Centered data\n",
    "            X = Y - mu\n",
    "\n",
    "            # If no disagreement, stop\n",
    "            if X.norm() < 1e-6:\n",
    "                break\n",
    "\n",
    "            # Empirical covariance (rank-deficient-safe)\n",
    "            Sigma = (X.T @ X) / max(len(Y) - 1, 1)\n",
    "\n",
    "            # Diagonal regularization\n",
    "            Sigma = Sigma + eps * torch.eye(C, device=device)\n",
    "\n",
    "            # Eigendecomposition with safety\n",
    "            try:\n",
    "                eigvals, eigvecs = torch.linalg.eigh(Sigma)\n",
    "            except RuntimeError:\n",
    "                # Covariance too ill-conditioned → skip trimming\n",
    "                break\n",
    "\n",
    "            lambda_star = eigvals[-1]\n",
    "\n",
    "            # If largest eigenvalue small enough, stop trimming\n",
    "            if lambda_star <= lambda_thresh:\n",
    "                break\n",
    "\n",
    "            # Principal direction\n",
    "            v_star = eigvecs[:, -1]\n",
    "\n",
    "            # Project samples onto principal direction\n",
    "            projections = torch.abs((Y - mu) @ v_star)\n",
    "\n",
    "            max_proj = projections.max()\n",
    "            if max_proj < 1e-6:\n",
    "                break\n",
    "\n",
    "            # Randomized trimming threshold (Cronus)\n",
    "            T = torch.sqrt(torch.rand(1, device=device)) * max_proj\n",
    "\n",
    "            mask = projections < T\n",
    "\n",
    "            # If too few samples left, stop\n",
    "            if mask.sum() <= 1:\n",
    "                break\n",
    "\n",
    "            # Trim and recompute mean\n",
    "            Y = Y[mask]\n",
    "            mu = Y.mean(dim=0)\n",
    "\n",
    "        agg[n] = mu\n",
    "\n",
    "    return agg\n",
    "\n",
    "def f_trimmed_mean(\n",
    "    logits,\n",
    "    beta=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Coordinate-wise trimmed mean aggregation.\n",
    "\n",
    "    logits: Tensor [K, N, C]  (models × samples × classes)\n",
    "    beta: fraction to trim from each side (0 < beta < 0.5)\n",
    "\n",
    "    returns: Tensor [N, C]\n",
    "    \"\"\"\n",
    "\n",
    "    K, N, C = logits.shape\n",
    "    device = logits.device\n",
    "\n",
    "    # Number to trim on each side\n",
    "    t = int(beta * K)\n",
    "    if 2 * t >= K:\n",
    "        raise ValueError(\"beta too large: removes all samples\")\n",
    "\n",
    "    agg = torch.zeros(N, C, device=device)\n",
    "\n",
    "    for n in range(N):\n",
    "        # Y: [K, C] logits for sample n\n",
    "        Y = logits[:, n, :]  # (K, C)\n",
    "\n",
    "        # Sort per coordinate\n",
    "        Y_sorted, _ = torch.sort(Y, dim=0)  # (K, C)\n",
    "\n",
    "        # Trim extremes\n",
    "        Y_trimmed = Y_sorted[t:K - t, :]  # (K - 2t, C)\n",
    "\n",
    "        # Mean of remaining values\n",
    "        agg[n] = Y_trimmed.mean(dim=0)\n",
    "\n",
    "    return agg\n",
    "\n",
    "def f_cronus_trimmed(\n",
    "    logits,\n",
    "    eps=1e-3,\n",
    "    lambda_thresh=9.0,\n",
    "    max_iters=5,\n",
    "    trim_frac=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid Cronus + coordinate-wise trimmed mean aggregation.\n",
    "\n",
    "    logits: Tensor [K, N, C]  (models × samples × classes)\n",
    "    trim_frac: fraction to trim in coordinate-wise step\n",
    "    returns: Tensor [N, C]\n",
    "    \"\"\"\n",
    "    K, N, C = logits.shape\n",
    "    device = logits.device\n",
    "    agg = torch.zeros(N, C, device=device)\n",
    "\n",
    "    for n in range(N):\n",
    "        # Step 1: take logits for this sample\n",
    "        Y = logits[:, n, :]\n",
    "\n",
    "        # --- Directional trimming (Cronus) ---\n",
    "        mu = Y.mean(dim=0)\n",
    "        for _ in range(max_iters):\n",
    "            X = Y - mu\n",
    "            if X.norm() < 1e-6:\n",
    "                break\n",
    "\n",
    "            Sigma = (X.T @ X) / max(len(Y)-1, 1)\n",
    "            Sigma = Sigma + eps * torch.eye(C, device=device)\n",
    "\n",
    "            try:\n",
    "                eigvals, eigvecs = torch.linalg.eigh(Sigma)\n",
    "            except RuntimeError:\n",
    "                break\n",
    "\n",
    "            lambda_star = eigvals[-1]\n",
    "            if lambda_star <= lambda_thresh:\n",
    "                break\n",
    "\n",
    "            v_star = eigvecs[:, -1]\n",
    "            projections = torch.abs((Y - mu) @ v_star)\n",
    "            max_proj = projections.max()\n",
    "            if max_proj < 1e-6:\n",
    "                break\n",
    "\n",
    "            T = torch.sqrt(torch.rand(1, device=device)) * max_proj\n",
    "            mask = projections < T\n",
    "            if mask.sum() <= 1:\n",
    "                break\n",
    "\n",
    "            Y = Y[mask]\n",
    "            mu = Y.mean(dim=0)\n",
    "\n",
    "        # --- Coordinate-wise trimming ---\n",
    "        K_remaining = Y.shape[0]\n",
    "        if K_remaining > 1:\n",
    "            lower_idx = int(K_remaining * trim_frac)\n",
    "            upper_idx = K_remaining - lower_idx\n",
    "\n",
    "            Y_sorted, _ = torch.sort(Y, dim=0)\n",
    "            Y = Y_sorted[lower_idx:upper_idx, :]  # trim top/bottom\n",
    "\n",
    "        # --- Aggregate ---\n",
    "        agg[n] = Y.mean(dim=0)\n",
    "\n",
    "    return agg\n",
    "\n",
    "# -----------------------------\n",
    "# Cronus + Geometric Median Aggregation\n",
    "# -----------------------------\n",
    "def f_cronus_geommedian(\n",
    "    logits,\n",
    "    eps=1e-3,\n",
    "    lambda_thresh=9.0,\n",
    "    max_iters=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid Cronus + geometric median aggregation.\n",
    "\n",
    "    logits: Tensor [K, N, C]  (models × samples × classes)\n",
    "    returns: Tensor [N, C]\n",
    "    \"\"\"\n",
    "    K, N, C = logits.shape\n",
    "    device = logits.device\n",
    "    agg = torch.zeros(N, C, device=device)\n",
    "\n",
    "    for n in range(N):\n",
    "        Y = logits[:, n, :]  # logits for sample n\n",
    "\n",
    "        # --- Cronus directional trimming ---\n",
    "        mu = Y.mean(dim=0)\n",
    "        for _ in range(max_iters):\n",
    "            X = Y - mu\n",
    "            if X.norm() < 1e-6:\n",
    "                break\n",
    "\n",
    "            Sigma = (X.T @ X) / max(len(Y)-1, 1)\n",
    "            Sigma = Sigma + eps * torch.eye(C, device=device)\n",
    "\n",
    "            try:\n",
    "                eigvals, eigvecs = torch.linalg.eigh(Sigma)\n",
    "            except RuntimeError:\n",
    "                break\n",
    "\n",
    "            lambda_star = eigvals[-1]\n",
    "            if lambda_star <= lambda_thresh:\n",
    "                break\n",
    "\n",
    "            v_star = eigvecs[:, -1]\n",
    "            projections = torch.abs((Y - mu) @ v_star)\n",
    "            max_proj = projections.max()\n",
    "            if max_proj < 1e-6:\n",
    "                break\n",
    "\n",
    "            T = torch.sqrt(torch.rand(1, device=device)) * max_proj\n",
    "            mask = projections < T\n",
    "            if mask.sum() <= 1:\n",
    "                break\n",
    "\n",
    "            Y = Y[mask]\n",
    "            mu = Y.mean(dim=0)\n",
    "\n",
    "        # --- Aggregate with geometric median ---\n",
    "        agg[n] = geometric_median(Y)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f47568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# BiLA-CM Aggregation Model\n",
    "# =============================\n",
    "class BilaModel(nn.Module):\n",
    "    def __init__(self, initLabels, lr=0.001, decay=0.5,\n",
    "                 t=1, hidden_layers=64, eps=1e-6):\n",
    "        \"\"\"\n",
    "        BiLA-CM label aggregation model.\n",
    "        initLabels: Tensor [K, N, C] initial labels from K models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        self.t = t\n",
    "\n",
    "        # Dimensions\n",
    "        K, N, C = initLabels.shape\n",
    "\n",
    "        # α: label correction network\n",
    "        self.W1 = nn.Linear(K * C, hidden_layers)\n",
    "        self.W2 = nn.Linear(hidden_layers, C)\n",
    "\n",
    "        # β: full confusion tensor (C × K × C)\n",
    "        w_init = torch.zeros(C, K, C)\n",
    "        for c in range(C):\n",
    "            for k in range(K):\n",
    "                w_init[c, k] = initLabels[k].mean(dim=0)\n",
    "\n",
    "        w_init = w_init / (w_init.sum(dim=2, keepdim=True) + eps)\n",
    "        self.w = nn.Parameter(w_init)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training & Aggregation\n",
    "    # -----------------------------\n",
    "    def UpdateAndAggregate(self, labels):\n",
    "        \"\"\"\n",
    "        labels: Tensor [K, N, C]\n",
    "        returns: Tensor [N, C] aggregated soft labels\n",
    "        \"\"\"\n",
    "        K, N, C = labels.shape\n",
    "        device = labels.device\n",
    "\n",
    "        for epoch in range(self.t):\n",
    "            psi = torch.softmax(self.w, dim=2)   # (C, K, C)\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for n in range(N):\n",
    "                label = labels[:, n, :]            # (K, C)\n",
    "                label_flat = label.reshape(-1)     # (K*C)\n",
    "\n",
    "                # q_alpha(y | l_i)\n",
    "                h = self.W2(torch.tanh(self.W1(label_flat)))  # (C,)\n",
    "                q_alpha = torch.softmax(h, dim=0)             # (C,)\n",
    "\n",
    "                # log g_beta(l_i | y=c)\n",
    "                l_idx = label.argmax(dim=1)        # (K,)\n",
    "                psi_ck_li = psi[:, torch.arange(K), l_idx]   # (C, K)\n",
    "                log_g_beta = torch.log(psi_ck_li + 1e-9).sum(dim=1)  # (C,)\n",
    "\n",
    "                # loss\n",
    "                loss_n = torch.sum(q_alpha * (torch.log(q_alpha + 1e-9) + log_g_beta))\n",
    "                total_loss -= loss_n\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # ---- aggregation ----\n",
    "        psi = torch.softmax(self.w, dim=2)        # (C, K, C)\n",
    "        c_idx = torch.arange(C, device=psi.device)\n",
    "        pi = psi[c_idx, :, c_idx]                 # (C, K)\n",
    "        labels_perm = labels.permute(2, 0, 1)     # (C, K, N)\n",
    "        scores = (pi.unsqueeze(2) * labels_perm).sum(dim=1)  # (C, N)\n",
    "        agg_labels = torch.softmax(scores.T, dim=1)           # (N, C)\n",
    "\n",
    "        return agg_labels\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utilities\n",
    "# =============================\n",
    "@torch.no_grad()\n",
    "def predict_logits(model, X):\n",
    "    \"\"\"\n",
    "    Returns logits from a model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    return model(X)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Geometric Median\n",
    "# =============================\n",
    "def geometric_median(Y, eps=1e-5, max_iters=10):\n",
    "    \"\"\"\n",
    "    Computes the geometric median of vectors Y ∈ R^(K × C) using Weiszfeld's algorithm.\n",
    "    \"\"\"\n",
    "    device = Y.device\n",
    "    mu = Y.mean(dim=0)  # initial guess\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        diff = Y - mu  # (K, C)\n",
    "        dist = torch.clamp(diff.norm(dim=1), min=1e-12)\n",
    "        weights = 1.0 / dist\n",
    "        mu_new = (weights[:, None] * Y).sum(dim=0) / weights.sum()\n",
    "        if (mu_new - mu).norm() < eps:\n",
    "            break\n",
    "        mu = mu_new\n",
    "\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8753b47-3faa-4acc-b8a4-2d689cd14a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Initialization\n",
    "observer = PublicPredictionObserver(num_classes=10, device=device)\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "mnist_test  = datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "X_train = mnist_train.data[:50000].float().flatten(1).to(device)\n",
    "Y_train = mnist_train.targets[:50000].to(device)\n",
    "\n",
    "X_pub = mnist_train.data[50000:].float().flatten(1).to(device)\n",
    "Y_pub = mnist_train.targets[50000:].to(device)\n",
    "\n",
    "X_test = mnist_test.data.float().flatten(1).to(device)\n",
    "Y_test = mnist_test.targets.to(device)\n",
    "\n",
    "NUM_PARTIES = 28       \n",
    "T1 = 50           \n",
    "T2 = 100             \n",
    "eps_adv = 0.1           \n",
    "lambda_thresh = 9.0 \n",
    "agg_iters = 2   \n",
    "NUM_LIE = 0\n",
    "NUM_LABEL_FLIP = 5\n",
    "LABEL_FLIP_TARGET = 5\n",
    "\n",
    "if NUM_LIE > 0 and NUM_LABEL_FLIP > 0:\n",
    "    raise Exception(\"Cannot have LIE and Label Flip attacks at the same time\")\n",
    "     \n",
    "# Split private data across parties (simple IID split)\n",
    "per_party = len(X_train) // NUM_PARTIES\n",
    "perm = torch.randperm(len(X_train), device=device)\n",
    "\n",
    "X_parts = []\n",
    "Y_parts = []\n",
    "for i in range(NUM_PARTIES):\n",
    "    idx = perm[i * per_party : (i + 1) * per_party]\n",
    "    X_parts.append(X_train[idx])\n",
    "    Y_parts.append(Y_train[idx])\n",
    "\n",
    "if NUM_LABEL_FLIP > 0:\n",
    "    for i in range(NUM_LABEL_FLIP):\n",
    "        Y_parts[i][:] = LABEL_FLIP_TARGET \n",
    "\n",
    "if NUM_LIE > 0:\n",
    "    models = [LIE_Model(active_round=1, num_malicious=NUM_LIE, num_models=NUM_PARTIES, observer=observer).to(device) for _ in range(0,NUM_LIE)]\n",
    "    models = models + [MnistModel().to(device) for _ in range(NUM_LIE,NUM_PARTIES)]\n",
    "else:\n",
    "    models =  [MnistModel().to(device) for _ in range(NUM_LIE,NUM_PARTIES)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b571e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialization phase (private-only, Adam lr=0.0005) :contentReference[oaicite:11]{index=11}\n",
    "for i in range(NUM_PARTIES):\n",
    "    opt = Adam(models[i].parameters(), lr=5e-4)\n",
    "    train_ce_fullbatch(models[i], X_parts[i], Y_parts[i], opt, epochs=T1)\n",
    "\n",
    "# Initial predictions on public set (Y^0_i = PREDICT(theta_i; Xp)) :contentReference[oaicite:12]{index=12}\n",
    "with torch.no_grad():\n",
    "    probs_stack = torch.stack([predict_probs(m, X_pub) for m in models], dim=0)  # (K,N,C)\n",
    "#Y_bar = f_cronus(probs_stack, eps=eps_adv, lambda_thresh=lambda_thresh)\n",
    "Y_bar = f_cronus_geommedian(probs_stack)\n",
    "observer.observe(Y_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dba79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cronus] epoch 00 | public error 0.0593\n",
      "Collab epoch 00, party 0, error 0.9130\n",
      "Collab epoch 00, party 1, error 0.8980\n",
      "Collab epoch 00, party 2, error 0.8657\n",
      "Collab epoch 00, party 3, error 0.8976\n",
      "Collab epoch 00, party 4, error 0.9115\n",
      "Collab epoch 00, party 5, error 0.1136\n",
      "Collab epoch 00, party 6, error 0.1131\n",
      "Collab epoch 00, party 7, error 0.1099\n",
      "Collab epoch 00, party 8, error 0.1236\n",
      "Collab epoch 00, party 9, error 0.1190\n",
      "Collab epoch 00, party 10, error 0.1164\n",
      "Collab epoch 00, party 11, error 0.1144\n",
      "Collab epoch 00, party 12, error 0.1129\n",
      "Collab epoch 00, party 13, error 0.1163\n",
      "Collab epoch 00, party 14, error 0.1171\n",
      "Collab epoch 00, party 15, error 0.1109\n",
      "Collab epoch 00, party 16, error 0.1243\n",
      "Collab epoch 00, party 17, error 0.1096\n",
      "Collab epoch 00, party 18, error 0.1083\n",
      "Collab epoch 00, party 19, error 0.1089\n",
      "Collab epoch 00, party 20, error 0.1188\n",
      "Collab epoch 00, party 21, error 0.1130\n",
      "Collab epoch 00, party 22, error 0.1196\n",
      "Collab epoch 00, party 23, error 0.1147\n",
      "Collab epoch 00, party 24, error 0.1071\n",
      "Collab epoch 00, party 25, error 0.1240\n",
      "Collab epoch 00, party 26, error 0.1164\n",
      "Collab epoch 00, party 27, error 0.1147\n",
      "[Cronus] epoch 01 | public error 0.0598\n",
      "Collab epoch 01, party 0, error 0.8177\n",
      "Collab epoch 01, party 1, error 0.8749\n",
      "Collab epoch 01, party 2, error 0.8787\n",
      "Collab epoch 01, party 3, error 0.8656\n",
      "Collab epoch 01, party 4, error 0.9008\n",
      "Collab epoch 01, party 5, error 0.1137\n",
      "Collab epoch 01, party 6, error 0.1114\n",
      "Collab epoch 01, party 7, error 0.1094\n",
      "Collab epoch 01, party 8, error 0.1237\n",
      "Collab epoch 01, party 9, error 0.1183\n",
      "Collab epoch 01, party 10, error 0.1147\n",
      "Collab epoch 01, party 11, error 0.1142\n",
      "Collab epoch 01, party 12, error 0.1122\n",
      "Collab epoch 01, party 13, error 0.1153\n",
      "Collab epoch 01, party 14, error 0.1159\n",
      "Collab epoch 01, party 15, error 0.1098\n",
      "Collab epoch 01, party 16, error 0.1237\n",
      "Collab epoch 01, party 17, error 0.1090\n",
      "Collab epoch 01, party 18, error 0.1084\n",
      "Collab epoch 01, party 19, error 0.1092\n",
      "Collab epoch 01, party 20, error 0.1179\n",
      "Collab epoch 01, party 21, error 0.1115\n",
      "Collab epoch 01, party 22, error 0.1188\n",
      "Collab epoch 01, party 23, error 0.1143\n",
      "Collab epoch 01, party 24, error 0.1067\n",
      "Collab epoch 01, party 25, error 0.1234\n",
      "Collab epoch 01, party 26, error 0.1169\n",
      "Collab epoch 01, party 27, error 0.1115\n",
      "[Cronus] epoch 02 | public error 0.0593\n",
      "Collab epoch 02, party 0, error 0.8617\n",
      "Collab epoch 02, party 1, error 0.8820\n",
      "Collab epoch 02, party 2, error 0.7367\n",
      "Collab epoch 02, party 3, error 0.8276\n",
      "Collab epoch 02, party 4, error 0.8762\n",
      "Collab epoch 02, party 5, error 0.1140\n",
      "Collab epoch 02, party 6, error 0.1104\n",
      "Collab epoch 02, party 7, error 0.1100\n",
      "Collab epoch 02, party 8, error 0.1227\n",
      "Collab epoch 02, party 9, error 0.1167\n",
      "Collab epoch 02, party 10, error 0.1148\n",
      "Collab epoch 02, party 11, error 0.1141\n",
      "Collab epoch 02, party 12, error 0.1122\n",
      "Collab epoch 02, party 13, error 0.1147\n",
      "Collab epoch 02, party 14, error 0.1139\n",
      "Collab epoch 02, party 15, error 0.1100\n",
      "Collab epoch 02, party 16, error 0.1234\n",
      "Collab epoch 02, party 17, error 0.1079\n",
      "Collab epoch 02, party 18, error 0.1077\n",
      "Collab epoch 02, party 19, error 0.1086\n",
      "Collab epoch 02, party 20, error 0.1183\n",
      "Collab epoch 02, party 21, error 0.1110\n",
      "Collab epoch 02, party 22, error 0.1189\n",
      "Collab epoch 02, party 23, error 0.1140\n",
      "Collab epoch 02, party 24, error 0.1061\n",
      "Collab epoch 02, party 25, error 0.1233\n",
      "Collab epoch 02, party 26, error 0.1163\n",
      "Collab epoch 02, party 27, error 0.1114\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Collaboration Phase\n",
    "# =============================\n",
    "Epoch = []\n",
    "public_err = []\n",
    "\n",
    "# Create an optimizer for each model (SGD on public + Adam on private if needed)\n",
    "opts = [torch.optim.SGD(models[i].parameters(), lr=1e-3) for i in range(NUM_PARTIES)]\n",
    "\n",
    "for t in range(T2):\n",
    "    # -----------------------------\n",
    "    # Each party trains on private + public data\n",
    "    # -----------------------------\n",
    "    for i in range(NUM_PARTIES):\n",
    "        # Train on private data\n",
    "        train_ce_fullbatch(models[i], X_parts[i], Y_parts[i], opts[i], epochs=1)\n",
    "        # Knowledge distillation / public data\n",
    "        train_distill_fullbatch(models[i], X_pub, Y_bar.detach(), opts[i], epochs=1)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Server Aggregation on public data\n",
    "    # -----------------------------\n",
    "    with torch.no_grad():\n",
    "        # Collect logits from all models\n",
    "        logits_stack = torch.stack([predict_logits(m, X_pub) for m in models], dim=0)  # (K, N, C)\n",
    "        # Convert to probabilities\n",
    "        probs_stack = F.softmax(logits_stack, dim=-1)\n",
    "        # Aggregate using Cronus (or Geometric Median if you prefer)\n",
    "        Y_bar = f_cronus_geommedian(probs_stack, eps=eps_adv, lambda_thresh=lambda_thresh)\n",
    "        #Y_bar = f_cronus(probs_stack, eps=eps_adv, lambda_thresh=lambda_thresh)\n",
    "        # Observe current aggregation (for monitoring)\n",
    "        observer.observe(logits_stack.mean(dim=0))\n",
    "\n",
    "    # -----------------------------\n",
    "    # Logging public error\n",
    "    # -----------------------------\n",
    "    with torch.no_grad():\n",
    "        cronus_preds = Y_bar.argmax(dim=1)\n",
    "        cronus_err = (cronus_preds != Y_pub).float().mean().item()\n",
    "        print(f\"[Cronus] epoch {t:02d} | public error {cronus_err:.4f}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Logging model error\n",
    "    # -----------------------------\n",
    "    for i in range(NUM_PARTIES):\n",
    "        preds = models[i](X_test).argmax(dim=1)\n",
    "        err = (preds != Y_test).float().mean().item()\n",
    "        print(f\"Collab epoch {t:02d}, party {i}, error {err:.4f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Update malicious/lying models rounds\n",
    "    # -----------------------------\n",
    "    for model in models[:NUM_LIE]:\n",
    "        model.num_rounds += 1\n",
    "\n",
    "    # -----------------------------\n",
    "    # Record metrics\n",
    "    # -----------------------------\n",
    "    Epoch.append(t)\n",
    "    public_err.append(cronus_err)\n",
    "\n",
    "print(\"Finished Collaboration Phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644a840-e90f-48a5-8a2c-a29fa553e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialization phase with bila (private-only, Adam lr=0.0005) :contentReference[oaicite:11]{index=11}\n",
    "for i in range(NUM_PARTIES):\n",
    "    opt = Adam(models[i].parameters(), lr=5e-4)\n",
    "    train_ce_fullbatch(models[i], X_parts[i], Y_parts[i], opt, epochs=T1)\n",
    "\n",
    "# Initial predictions on public set (Y^0_i = PREDICT(theta_i; Xp)) :contentReference[oaicite:12]{index=12}\n",
    "with torch.no_grad():\n",
    "    probs_stack = torch.stack([predict_probs(m, X_pub) for m in models], dim=0)  # (K,N,C)\n",
    "bilaAggregator = BilaModel(probs_stack).to(device)\n",
    "Y_bar = bilaAggregator.UpdateAndAggregate(probs_stack)\n",
    "observer.observe(Y_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf4202-deb4-42a9-a648-4852b02e30f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Collaboration phase (with bila)\n",
    "\n",
    "opts = []\n",
    "for i in range(NUM_PARTIES):\n",
    "    # Create an SGD optimizer for each model\n",
    "    opts.append(SGD(models[i].parameters(), lr=1e-3))\n",
    "\n",
    "for t in range(T2):\n",
    "    # Each party updates on Di ∪ Dp (paper does private Adam + public SGD) :contentReference[oaicite:13]{index=13}\n",
    "    for i in range(NUM_PARTIES):\n",
    "        train_ce_fullbatch(models[i], X_parts[i], Y_parts[i], opts[i], epochs=1)\n",
    "        train_distill_fullbatch(models[i], X_pub, Y_bar.detach(), opts[i], epochs=1)\n",
    "\n",
    "    # Parties send prediction vectors on Xp; server aggregates to Y_bar^{t+1}\n",
    "    with torch.no_grad():\n",
    "        probs_stack = torch.stack([predict_probs(m, X_pub) for m in models], dim=0)\n",
    "\n",
    "    Y_bar = bilaAggregator.UpdateAndAggregate(probs_stack)\n",
    "\n",
    "    observer.observe(Y_bar)\n",
    "\n",
    "    # Logging\n",
    "    with torch.no_grad():\n",
    "        for i in range(NUM_PARTIES):\n",
    "            preds = models[i](X_test).argmax(dim=1)\n",
    "            err = (preds != Y_test).float().mean().item()\n",
    "            print(f\"Collab epoch {t:02d}, party {i}, error {err:.4f}\")\n",
    "\n",
    "    for model in models[0:NUM_MALICIOUS]:\n",
    "        model.num_rounds+=1 \n",
    "\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
