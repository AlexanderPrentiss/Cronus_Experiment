{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6460dea",
   "metadata": {},
   "source": [
    "### Limit Scope to Only AGR Testing, No Sockets, No Implementation, Just Logits Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f47568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a49b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 9 # Try to keep 60,000 divisiable by NUM_MODELS + 1\n",
    "EPOCHS_PER_ROUND = 10\n",
    "model_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8592049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data fetching\n",
    "transform = transforms.ToTensor()\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# randomly split dataset into models\n",
    "# dataset 0 is the servers public data\n",
    "training_sizes = [len(mnist_trainset) // (NUM_MODELS + 1)] * (NUM_MODELS + 1)\n",
    "training_datasets = random_split(dataset=mnist_trainset, lengths=training_sizes)\n",
    "# Load testsset\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "training_loaders = [\n",
    "    DataLoader(training_datasets, batch_size=batch_size, shuffle=True)\n",
    "    for training_datasets in training_datasets\n",
    "]\n",
    "\n",
    "testing_loader = DataLoader(mnist_testset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8590c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE PUBLIC DATA SET\n",
    "class PublicDistillationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, soft_labels):\n",
    "        self.base = base_dataset\n",
    "        self.soft_labels = soft_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = self.base[idx]   # ignore original label\n",
    "        return x, self.soft_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CRONUS AGR\n",
    "def distillation_loss(student_logits, teacher_logits, T=1.0):\n",
    "    return F.kl_div(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T * T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BENIGN MODEL DEFINITION\n",
    "class MnistModel(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "      self.lin1 = nn.Linear(784, 256)\n",
    "      self.lin2 = nn.Linear(256, 64)\n",
    "      self.lin3 = nn.Linear(64, 10)\n",
    "\n",
    "  def forward(self, X):\n",
    "      x1 = F.relu(self.lin1(X))\n",
    "      x2 = F.relu(self.lin2(x1))\n",
    "      x3 = F.relu(self.lin3(x2))\n",
    "      return x3\n",
    "\n",
    "  # Fit function\n",
    "  def fit(self, dataloader, optimizer, loss_fn, epochs, public_loader = None):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y in dataloader:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = self(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      if public_loader is not None: # IF A PUBLIC DATA LOADER IS PRESENT USE DISTILATION LOSS WITH PUBLIC DATA (BECAUSE IT ISNT JUST ONE INT)\n",
    "        for x, y in public_loader:\n",
    "          x = x.view(x.size(0), -1)\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          preds = self(x)\n",
    "          loss = distillation_loss(preds, y)\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "#TODO: Malicious Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Training model 1\n",
      "Training model 2\n",
      "Training model 3\n",
      "Training model 4\n",
      "Training model 5\n",
      "Training model 6\n",
      "Training model 7\n",
      "Training model 8\n",
      "Training model 9\n"
     ]
    }
   ],
   "source": [
    "# INITIAL TRAINING\n",
    "models = []\n",
    "models.append(None)\n",
    "for i in range(1, NUM_MODELS + 1):\n",
    "    print(i)\n",
    "    models.append(MnistModel())\n",
    "# Adam for local training phase and first 50 epochs of collaborative phase\n",
    "# SGD is for the last 50 epochs of the collaborative phase\n",
    "\n",
    "# Loss function is same for all epochs\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#FIRST ROUND\n",
    "for i in range(1, NUM_MODELS + 1):\n",
    "    print(f\"Training model {i}\")\n",
    "\n",
    "    models[i].fit(\n",
    "        dataloader=training_loaders[i],\n",
    "        optimizer=SGD(models[i].parameters(), lr=1e-3),\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=EPOCHS_PER_ROUND\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b650d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from model 1\n",
      "\n",
      "Getting predictions from model 2\n",
      "\n",
      "Getting predictions from model 3\n",
      "\n",
      "Getting predictions from model 4\n",
      "\n",
      "Getting predictions from model 5\n",
      "\n",
      "Getting predictions from model 6\n",
      "\n",
      "Getting predictions from model 7\n",
      "\n",
      "Getting predictions from model 8\n",
      "\n",
      "Getting predictions from model 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INITIAL Predictions\n",
    "for i in range(1, NUM_MODELS + 1):\n",
    "    models[i].eval()\n",
    "\n",
    "all_model_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1, NUM_MODELS + 1):\n",
    "        print(f'Getting predictions from model {i}\\n')\n",
    "        model = models[i]\n",
    "        model_preds = []\n",
    "\n",
    "        for x, _ in training_loaders[0]:   # labels optional\n",
    "            x = x.view(x.size(0), -1)\n",
    "            preds = model(x)      # logits\n",
    "            model_preds.append(preds)\n",
    "\n",
    "        # shape: (num_agr_samples, 10)\n",
    "        model_preds = torch.cat(model_preds, dim=0)\n",
    "        all_model_preds.append(model_preds)\n",
    "\n",
    "all_model_preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGR FOR LOGITS AND CUSTOM DATA LOADER TO PAS TO FIT\n",
    "\n",
    "def cronus_robust_mean(\n",
    "    Y,              # Tensor [n_models, num_classes]\n",
    "    eps=0.2,        # fraction of adversaries (<= 0.5)\n",
    "    iters=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements Cronus aggregation for ONE public sample.\n",
    "    Y: logits from all models for one datapoint\n",
    "    \"\"\"\n",
    "    S = Y.clone()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        mu = S.mean(dim=0)\n",
    "        centered = S - mu\n",
    "\n",
    "        # covariance\n",
    "        cov = centered.T @ centered / S.size(0)\n",
    "\n",
    "        # top eigenvector\n",
    "        eigvals, eigvecs = torch.linalg.eigh(cov)\n",
    "        v = eigvecs[:, -1]          # largest eigenvalue direction\n",
    "        lam = eigvals[-1]\n",
    "\n",
    "        # stopping condition (paper uses threshold 9)\n",
    "        if lam <= 9:\n",
    "            break\n",
    "\n",
    "        # project onto principal direction\n",
    "        proj = torch.abs(centered @ v)\n",
    "\n",
    "        # trim eps/2 fraction (deterministic)\n",
    "        keep = int((1 - eps/2) * S.size(0))\n",
    "        _, idx = torch.topk(proj, keep, largest=False)\n",
    "        S = S[idx]\n",
    "\n",
    "    return S.mean(dim=0)\n",
    "\n",
    "def cronus_aggregate_all(\n",
    "    all_model_preds,   # list of [num_public, num_classes]\n",
    "    eps=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns aggregated soft labels Y_bar:\n",
    "    Tensor [num_public, num_classes]\n",
    "    \"\"\"\n",
    "    n_models = len(all_model_preds)\n",
    "    num_public, num_classes = all_model_preds[0].shape\n",
    "\n",
    "    Y_bar = torch.zeros(num_public, num_classes)\n",
    "\n",
    "    for k in range(num_public):\n",
    "        # collect predictions for datapoint k\n",
    "        Yk = torch.stack([all_model_preds[i][k] for i in range(n_models)])\n",
    "        Y_bar[k] = cronus_robust_mean(Yk, eps=eps)\n",
    "\n",
    "    return Y_bar\n",
    "\n",
    "\n",
    "Y_bar = cronus_aggregate_all(all_model_preds, eps=0.2)\n",
    "public_dataset = PublicDistillationDataset(\n",
    "    training_datasets[0],  # server/public split\n",
    "    Y_bar\n",
    ")\n",
    "\n",
    "public_loader = DataLoader(\n",
    "    public_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e21f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Training model 2\n",
      "Training model 3\n",
      "Training model 4\n",
      "Training model 5\n",
      "Training model 6\n",
      "Training model 7\n",
      "Training model 8\n",
      "Training model 9\n"
     ]
    }
   ],
   "source": [
    "#ROUND 2 TRAINING... TODO: Inf\n",
    "for i in range(1, NUM_MODELS + 1):\n",
    "    print(f\"Training model {i}\")\n",
    "\n",
    "    models[i].fit(\n",
    "        dataloader=training_loaders[i],\n",
    "        optimizer=SGD(models[i].parameters(), lr=1e-3),\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=EPOCHS_PER_ROUND,\n",
    "        public_loader = public_loader\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Honors_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
